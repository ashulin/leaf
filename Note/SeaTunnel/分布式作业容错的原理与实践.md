# 分布式作业容错的原理与实践

## 引言

对于分布式流处理系统而言，高吞吐、低延迟往往是最主要的需求。与此同时，容错在分布式系统中也很重要，对于正确性要求较高的场景，exactly once的实现往往也非常重要。

在分布式流处理系统中，由于各个节点的算力、网络、负载等情况各有不同，每个节点的状态直接合并不能得到一个真正的全局状态。为了得到一致性的结果，分布式处理系统需要对节点的失败要有弹性，即失败时可以恢复到一致性的结果。

## 核心原理：Chandy-Lamport 算法

而Chandy-Lamport算法就是用来在缺乏类似全局时钟或者全局时钟不可靠的分布式系统中来确定一种全局状态。

即确定一个 Global 的 Snapshot，错误处理的时候各个节点根据上一次的 Global Snapshot 来恢复。

Chandy-Lamport 算法的执行可以和分布式系统的计算同时进行，不会影响和更改底层的运算状态，同时它要求系统的消息有序。

Chandy-Lamport 算法以两个作者的名字命名。

其中 Lamport 就是分布式领域先驱、图灵奖获得者 Leslie Lamport，著名的一致性算法 Paxos、全局时钟算法Lamport timestamps的作者。

算法的论文于 1985 年发表，[Distributed Snapshots: Determining Global States of a Distributed System](http://lamport.azurewebsites.net/pubs/pubs.html#time-clocks)

在 Chandy-Lamport 算法中，为了定义分布式系统的全局状态，我们先将分布式系统简化成有限个进程和进程之间的 channel 组成，也就是一个有向图：节点是进程，边是 channel。因为是分布式系统，也就是说，这些进程是运行在不同的物理机器上的。那么一个分布式系统的全局状态就是有进程的状态和 channel 中的 message 组成，这个也是分布式快照算法需要记录的。

因为是有向图，所以每个进程对应着两类 channel: input channel, output channel。同时假设 Channel 是一个容量无限大的 FIFO 队列，收到的 message 都是有序且无重复的。Chandy-Lamport 分布式快照算法通过记录每个进程的 local state 和它的 input channel 中有序的 message，我们可以认为这是一个局部快照。那么全局快照就可以通过将所有的进程的局部快照合并起来得到。

![图 1](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/fig-1.png)

那么我们基于上面假设的分布式系统模型来看一下 Chandy-Lamport 算法具体的工作流程是什么样的。主要包括下面三个部分：

-   Initiating a snapshot: 也就是开始创建 snapshot，可以由系统中的任意一个进程发起
-   Propagating a snapshot: 系统中其他进程开始逐个创建 snapshot 的过程
-   Terminating a snapshot: 算法结束条件

***Initiating a snapshot\***

-   进程 Pi 发起: 记录自己的进程状态，同时生产一个标识信息 marker，marker 和进程通信的 message 不同
-   将 marker 信息通过 ouput channel 发送给系统里面的其他进程
-   开始记录所有 input channel 接收到的 message

***Propagating a snapshot\***

-   对于进程 Pj 从 input channel Ckj 接收到 marker 信息:

-   如果 Pj 还没有记录自己的进程状态，则

-   -   Pj 记录自己的进程状态，同时将 channel Ckj 置为空
    
        >   Pj从Ckj收到marker说明Pj已经处理完了收到marker时间点之前的所有从Ckj收到的messages，只要记录Pj此时的状态就包含了之前从Ckj收到的所有messages
    
    -   向 output channel 发送 marker 信息
    
-   *否则*

-   -   记录其他 channel 在收到 marker 之前的 channel 中收到所有 message

所以这里的 marker 其实是充当一个分隔符，分隔进程做 local snapshot （记录进程状态）的 message。比如 Pj 做完 local snapshot 之后 Ckj 中发送过来的 message 为 [a,b,c,marker,x,y,z] 那么 a, b, c 就是进程 Pk 做 local snapshot 前的数据，Pj 对于这部分数据需要记录下来，比如记录在 log 里面。而 marker 后面 message 正常处理掉就可以了。

***Terminating a snapshot\***

-   所有的进程都收到 marker 信息并且记录下自己的状态和 channel 的状态（包含的 message）

## 现有实践中的痛点

Spark 的 Structured Streaming 虽然在官方博客中披露使用的 Chandy-Lamport 算法来做 Failover 处理，但是并没有更细节的披露。

Flink基于以上算法实现了Checkpoint作为容错机制，并发表了相关论文：[Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/abs/1506.08603)

### Global Failover

在目前的工业实现中，当作业出错后需要作业DAG的所有节点进行failover，整个过程持续的时间会比较长，会导致上游累积很多数据。

## Apache SeaTunnel 的容错实践

### Pipeline Failover

在数据集成场景中，存在一个Job同步上百张的可能，一个节点/一张表的失败而导致所有表失败这样的代价太大。

我们期望没有相关性的Job Task不会在容错时相互影响，所以我们将一个有上下游关系的vertex集合称为Pipeline，一个Job可以由一个或多个pipeline组成。

![Pipeline](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/Pipeline.png)

### Regional Failover

现在如果pipeline出现异常，我们仍然需要pipeline所有vertex进行failover；那么可不可以只恢复部分vertex呢？

比如Source出错，Sink不重启，单Source多Sink的情况，单个Sink出错，只恢复出错的Sink与Source；即只恢复出错的节点以及其上游节点。

显然，无状态vertex可以不重启，而由于SeaTunnel是数据集成框架，我们不存在Agg、Count等聚合状态vertex，所以只用考虑Sink；

-   Sink不支持幂等 & 2PC；不重启和重启都会导致同样的数据重复，只能由Sink解决该问题，可以不重启；
-   Sink支持幂等，不支持2PC：由于是幂等写入，所以Source每次是否读取数据不一致无所谓，可以不重启了；
-   Sink支持2PC：
    -   如果Source支持数据一致性，如果不执行abort，通过channel数据ID自动忽略已处理的老数据，同时会面临事务会话时间可能超时的问题；
    -   如果Source不支持数据一致性，对Sink执行abort放弃上一次的数据，这与重启的效果一样但不需要重新建立链接等初始化操作；
    -   即最简单的实现方式是执行abort；

由以上可以得出，该特性是可以实现的；

### Checkpoint 流程

简单来说，包括以下几个部分：

-   通过DAG生成Checkpoint计划；
-   当所有Vertex部署完成后Coordinator启动；
-   Coordinator 定时生成 Checkpoint，并发送给Source；
-   Source 收到 trigger checkpoint 的 PRC，自己开始做 snapshot，并往下游发送 barrier；
    -   Vertex 开始同步阶段 snapshot；
    -   Vertex 开始异步阶段 snapshot，并汇报给Coordinator；
-   下游接收 barrier，并作snapshot，直到Sink；
-   当所有Vertex都汇报给 Coordinator视为Checkpoint完成；

#### 生成 Checkpoint 计划

-   未优化的未并行化的算子ID记录，用于记录算子的状态；
-   起始点，用于触发CheckpointBarrier的流程；
-   所有Vertex，用于记录每个点的信息；

#### 启动  

-   [x] 情况1: Task deploy进入待恢复状态后通知Coordinator；

    -   [x] 恢复State，需判断Action并行度是否变更：
        -   [x] 组合Task所需的State，并发送给Task；
        -   [x] Task恢复状态；
            -   [x] ReaderTask将状态转发给SplitEnumeratorTask；
-   [x] 情况2: Task deploy进入待启动状态后通知Coordinator；

    -   [x] 需要所有Task进入待启动后再进行启动；

-   [x] 接收到所有startingTasks通知后Coordinator启动；
-   [x] Coordinator接收到所有Task通知后Task运行；

#### 创建 Checkpoint

-   定时触发PendingCheckpoint；
-   达到Checkpoint数量上限后，将在有PendingCheckpoint完成时再触发PendingCheckpoint；
-   用户手动触发Savepoint；

#### CheckpointBarrier 流程

-   CheckpointCoordinator发送TriggerOperation给SplitEnumeratorTask；

    -   SplitEnumeratorTask响应TriggerOperation；
        -   对Enumerator进行snapshot，获取state，对Coordinator进行ACK（Task在进行ACK时附带state）；
        -   发送TriggerOperation给ReaderTask；

    -   普通Task响应TriggerOperation，Barrier在Flow中流转;
        -   Flow接收到Barrier时如果存在Action则进行snapshot，获取state，返回给Task;
            -   SinkFlow执行Prepare Commit获取CommitInfo；
            -   SinkFlow在做完上述操作后，将发送PrepareCommitOperation（由TriggerOperation与CommitInfo组成）给AggregatedCommitterTask；
            -   PartitionTransformSourceFlow中存在Barrier的对齐操作；
        -   Flow接收到Barrier对Task做ACK；
        -   Task的所有Flow做ACK后，对Coordinator进行ACK；

-   AggregatedCommitterTask响应PrepareCommitOperation
    -   合并CommitInfo为AggregatedCommitInfo；
        -   Barrier对齐后对Coordinator进行ACK；
        -   进行commit；

#### 自动完成的 Checkpoint

由于Checkpoint时定时触发的，而对于一个有界的Source，它可能在两个Checkpoint之间完成数据的读取，如果等待触发，这会造成资源的浪费（不能及时释放资源）；

这时会由Source产生一个CompletedBarrier，并和CheckpointBarrier进行同样流程的流转，同时我们将该Checkpoint称为CompletedPoint；

## Apache SeaTunnel 展望

为一个Apache 孵化项目，Apache SeaTunnel 社区迅速发展，在接下来的社区规划中，主要有四个方向：

-   扩大与完善 Connector & Catalog 生态

    支持更多 Connector & Catalog，如TiDB、Doris、Stripe等，并完善现有的连接器，提高其可用性与性能等；

    支持CDC连接器，用于支持实时增量同步场景；

    >   对连接器感兴趣的同学可以关注该Umbrella：https://github.com/apache/incubator-seatunnel/issues/1946

-   支持引擎的更多版本

    如Spark 3.x, Flink 1.14.x等

    >   对支持Spark 3.3 感兴趣的同学可以关注该PR：https://github.com/apache/incubator-seatunnel/pull/2574

-   支持更多数据集成场景 （SeaTunnel Engine）

    用于解决整库同步、表结构变更同步、任务失败影响粒度大等现有引擎不能解决的痛点；

    >   对engine感兴趣的同学可以关注该Umbrella：https://github.com/apache/incubator-seatunnel/issues/2272

-   更简单易用（SeaTunnel Web）

    提供Web界面以DAG/SQL等方式使操作更简单，更加直观的展示Catalog、Connector、Job等；

    接入调度平台，使任务管理更简单；

    >   对Web 感兴趣的同学可以关注我们的Web子项目：https://github.com/apache/incubator-seatunnel-web

### Engine 动机

-   Schema Evolution：支持表结构的同步；
-   Shared resources：共享资源，如JDBC连接池等；
-   Read Sharding：支持分库分表的读取；
-   Speed Control：支持在未反压或反压无法传递到Source时，对Source进行读取速率的控制；
-   Data Cache：支持Sink出错时，将不可重启的Source数据写入到缓存中；
-   Finer fault tolerance：支持更小粒度的容错；

## 参考

1.   [Chi: A Scalable and Programmable Control Plane for Distributed Stream Processing Systems](https://shivaram.org/publications/chi-vldb18.pdf)
2.   [Time, Clocks and the Ordering of Events in a Distributed System](http://lamport.azurewebsites.net/pubs/pubs.html#time-clocks)
3.   [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/abs/1506.08603)
4.   [分布式快照算法: Chandy-Lamport 算法] (https://zhuanlan.zhihu.com/p/53482103
