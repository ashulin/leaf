# 分布式作业容错的原理与实践

## 引言

对于分布式流处理系统而言，高吞吐、低延迟往往是最主要的需求。与此同时，容错在分布式系统中也很重要，对于正确性要求较高的场景，exactly
once的实现往往也非常重要。

在分布式流处理系统中，由于各个节点的算力、网络、负载等情况各有不同，每个节点的状态直接合并不能得到一个真正的全局状态。为了得到一致性的结果，分布式处理系统需要对节点的失败要有弹性，即失败时可以恢复到一致性的结果。

全局一致性快照是可以用来给分布式系统做备份和故障恢复的机制。

它有几个特点：

- 一个分布式应用，它有多个进程分布在多个服务器上；
- 有状态，并且状态随着数据处理产生变化；
- 应用间可以相互通信，消息在channel内进行传递；
- 在某一时刻/事件的统一状态，包括各个进程的本地状态和传递的消息，并且事件的状态遵循happens-before；
- 也就是说如果事件B发生在事件A之前，那么事件B的状态就包含了事件A

它的作用：

- 用作Checkpoint，可以定期对全局状态做备份，故障时可以通过已完成的全局快照恢复到某一时刻；
- 判断处理逻辑是否死锁；
- 通过全局一致性进行小文件压缩等；

## 核心原理：Chandy-Lamport 算法

而Chandy-Lamport算法就是用来在缺乏类似全局时钟或者全局时钟不可靠的分布式系统中来确定一种全局状态。

即确定一个 Global 的 Snapshot，错误处理的时候各个节点根据上一次的 Global Snapshot 来恢复。

Chandy-Lamport 算法的执行可以和分布式系统的计算同时进行，不会影响和更改底层的运算状态，同时它要求系统的消息有序。

Chandy-Lamport 算法以两个作者的名字命名。

其中 Lamport 就是分布式领域先驱、图灵奖获得者 Leslie Lamport，著名的一致性算法 Paxos、全局时钟算法Lamport timestamps的作者。

算法的论文于 1985
年发表，[Distributed Snapshots: Determining Global States of a Distributed System](http://lamport.azurewebsites.net/pubs/pubs.html#time-clocks)

在 Chandy-Lamport 算法中，为了定义分布式系统的全局状态，我们先将分布式系统简化成有限个进程和进程之间的 channel
组成，也就是一个有向图：节点是进程，边是 channel。因为是分布式系统，也就是说，这些进程是运行在不同的物理机器上的。那么一个分布式系统的全局状态就是有进程的状态和
channel 中的 message 组成，这个也是分布式快照算法需要记录的。

因为是有向图，所以每个进程对应着两类 channel: input channel, output channel。同时假设 Channel 是一个容量无限大的 FIFO
队列，收到的 message 都是有序且无重复的。Chandy-Lamport 分布式快照算法通过记录每个进程的 local state 和它的 input channel
中有序的 message，我们可以认为这是一个局部快照。那么全局快照就可以通过将所有的进程的局部快照合并起来得到。

![图 1](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/fig-1.png)

那么我们基于上面假设的分布式系统模型来看一下 Chandy-Lamport 算法具体的工作流程是什么样的。主要包括下面三个部分：

- Initiating a snapshot: 也就是开始创建 snapshot，可以由系统中的任意一个进程发起
- Propagating a snapshot: 系统中其他进程开始逐个创建 snapshot 的过程
- Terminating a snapshot: 算法结束条件

***Initiating a snapshot\***

![snapshot-start](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/snapshot-start.png)

- 进程 Pi 发起: 记录自己的进程状态，同时生产一个标识信息 marker，marker 和进程通信的 message 不同
- 将 marker 信息通过 ouput channel 发送给系统里面的其他进程
- 开始记录所有 input channel 接收到的 message

***Propagating a snapshot\***

![snapshot-process](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/snapshot-process.png)

- 对于进程 Pj 从 input channel Ckj 接收到 marker 信息:

- 如果 Pj 还没有记录自己的进程状态，则

-
    - Pj 记录自己的进程状态，同时将 channel Ckj 置为空

      > Pj从Ckj收到marker说明Pj已经处理完了收到marker时间点之前的所有从Ckj收到的messages，只要记录Pj此时的状态就包含了之前从Ckj收到的所有messages
    
    - 向 output channel 发送 marker 信息

- *否则*

-
    - 记录其他 channel 在收到 marker 之前的 channel 中收到所有 message

所以这里的 marker 其实是充当一个分隔符，分隔进程做 local snapshot （记录进程状态）的 message。比如 Pj 做完 local snapshot 之后
Ckj 中发送过来的 message 为 [a,b,c,marker,x,y,z] 那么 a, b, c 就是进程 Pk 做 local snapshot 前的数据，Pj
对于这部分数据需要记录下来，比如记录在 log 里面。而 marker 后面 message 正常处理掉就可以了。

***Terminating a snapshot\***

- 所有的进程都收到 marker 信息并且记录下自己的状态和 channel 的状态（包含的 message）

## 现有实践中的痛点

Spark 的 Structured Streaming 虽然在官方博客中披露使用的 Chandy-Lamport 算法来做 Failover 处理，但是并没有更细节的披露。

Flink基于以上算法实现了Checkpoint作为容错机制，并发表了相关论文：[Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/abs/1506.08603)

### Global Failover

在目前的工业实现中，当作业出错后需要作业DAG的所有节点进行failover，整个过程持续的时间会比较长，会导致上游累积很多数据。

![global-failover](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/global-failover.png)

## Apache SeaTunnel 的容错实践

### Pipeline Failover

在数据集成场景中，存在一个Job同步上百张的可能，一个节点/一张表的失败而导致所有表失败这样的代价太大。

我们期望没有相关性的Job Task不会在容错时相互影响，所以我们将一个有上下游关系的vertex集合称为Pipeline，一个Job可以由一个或多个pipeline组成。

![pipeline-failover](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/pipeline-failover.png)

### Regional Failover

现在如果pipeline出现异常，我们仍然需要pipeline所有vertex进行failover；那么可不可以只恢复部分vertex呢？

![pipeline-failover](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/pipeline-failover.png)

比如Source出错，Sink不重启，单Source多Sink的情况，单个Sink出错，只恢复出错的Sink与Source；即只恢复出错的节点以及其上游节点。

显然，无状态vertex可以不重启，而由于SeaTunnel是数据集成框架，我们不存在Agg、Count等聚合状态vertex，所以只用考虑Sink；

- Sink不支持幂等 & 2PC；不重启和重启都会导致同样的数据重复，只能由Sink解决该问题，可以不重启；
- Sink支持幂等，不支持2PC：由于是幂等写入，所以Source每次是否读取数据不一致无所谓，可以不重启了；
- Sink支持2PC：
    - 如果Source支持数据一致性，如果不执行abort，通过channel数据ID自动忽略已处理的老数据，同时会面临事务会话时间可能超时的问题；
    - 如果Source不支持数据一致性，对Sink执行abort放弃上一次的数据，这与重启的效果一样但不需要重新建立链接等初始化操作；
    - 即最简单的实现方式是执行abort；

由以上可以得出，该特性是可以实现的；

### Checkpoint 流程

简单来说，包括以下几个部分：

- 通过DAG生成Checkpoint计划；
- 当所有Vertex部署完成后Coordinator启动；
- Coordinator 定时生成 Checkpoint，并发送给Source；
- Source 收到 trigger checkpoint 的 PRC，自己开始做 snapshot，并往下游发送 barrier；
    - Vertex 开始同步阶段 snapshot；
    - Vertex 开始异步阶段 snapshot，并汇报给Coordinator；
- 下游接收 barrier，并作snapshot，直到Sink；
- 当所有Vertex都汇报给 Coordinator视为Checkpoint完成；

#### 生成 Checkpoint 计划

![checkpoint-plan](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/checkpoint-plan.png)

- DAG算子ID记录，用于记录算子的状态；
- ExecutionGraph起始点，用于触发CheckpointBarrier的流程；
- 所有ExecutionGraph Vertex，用于记录每个点的信息；

#### CheckpointBarrier 流程

- Coordinator触发Checkpoint，让Barrier在DAG Flow中流转；

  ![checkpoint-trigger](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/checkpoint-trigger.png)

    1. Trigger starting vertex
    2. barrier对齐，然后对Vertex做snapshot，以及向output channel发送barrier，该步骤是原子的，且是同步的
    3. vertex发送ACK给Coordinator（包含状态/状态存储路径）

- barrier流转

  ![barrier-flow](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/barrier-flow.png)

    - SinkCommitter 还需要执行 PrepareCommit

- Checkpoint 完成

  ![checkpoint-completed](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/checkpoint-completed.png)

    - Coordinator 通知所有Vertex Checkpoint 完成
        - Source 提交Offset等信息
        - SinkCommitter执行Commit
        - SinkAggregatedCommitter执行Commit

### 自动完成的 Checkpoint

由于Checkpoint时定时触发的，而对于一个有界的Source，它可能在两个Checkpoint之间完成数据的读取，如果等待触发，这会造成资源的浪费（不能及时释放资源）；

这时会由Source产生一个CompletedBarrier，并和CheckpointBarrier进行同样流程的流转，同时我们将该Checkpoint称为CompletedPoint；

同时对于离线作业，可能需求只在离线作业完成时进行数据的提交，中途不需要checkpoint，也可由此达成；

### Aligned Barrier

与 Chandy-Lamport 不进行 Barrier 对齐相比，Barrier 对齐可以不对 input channel 的数据进行 snapshot

#### Exactly-Once

- barrier 对齐时，阻塞已读取到 barrier 的 input channel
- 当所有 input channel 的 barrier 都读取到时，进行 snapshot 后将 barrier发送到所有output channel中

![aligned-barrier](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/aligned-barrier.png)

#### At-Least-Once

- barrier 对齐时，不阻塞已读取到 barrier 的 input channel

- 当所有 input channel 的 barrier 都读取到时，进行 snapshot 后将 barrier发送到所有output channel中

- 由于没有阻塞 input channel，所以发送 barrier 到 output channel 时会将下一次 Checkpoint的数据包含到当前的 Checkpoint
  中，即数据可能重复（At-Least-Once）；

  ![aligned-barrier-2](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/aligned-barrier-2.png)

### Unaligned Barrier

> 这不适合 SeaTunnel 的场景，不会实现。

![unaligned-barrier-2](resources/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BD%9C%E4%B8%9A%E5%AE%B9%E9%94%99%E7%9A%84%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5/unaligned-barrier-2.png)

Unaligned Barrier需要对 channel 的数据进行 snapshot 以支持 Exactly-Once；

#### Apache Flink Unaligned

- 在 input channel 中读取到首个 barrier 时，将 barrier 插入到 output channel 的队首；
    - 对 vertex 进行 snapshot （同步）
    - 对当前所有output channel的数据 snapshot
- 记录剩余 input channel 的数据，直到所有 barrier 被读取 （异步）；

#### Chandy-Lamport Unaligned

- 在 input channel 中读取到首个 barrier 时，将 barrier 插入到 output channel 的队尾
- 记录剩余 input channel 的数据

两者对比，Flink需要多记录output channel的数据，然后将barrier插入到队首中，这样的好处是 barrier 可以尽快流转到 DAG 的所有
Vertex 中，以尽量减少 Checkpoint 的完成时间，使 Checkpoint 更稳定（实际的 Checkpoint 时间仍然由反压节点决定）；

## Apache SeaTunnel 展望

为一个Apache 孵化项目，Apache SeaTunnel 社区迅速发展，在接下来的社区规划中，主要有四个方向：

- 扩大与完善 Connector & Catalog 生态

  支持更多 Connector & Catalog，如TiDB、Doris、Stripe等，并完善现有的连接器，提高其可用性与性能等；

  支持CDC连接器，用于支持实时增量同步场景；

  > 对连接器感兴趣的同学可以关注该Umbrella：https://github.com/apache/incubator-seatunnel/issues/1946

- 支持引擎的更多版本

  如Spark 3.x, Flink 1.14.x等

  > 对支持Spark 3.3 感兴趣的同学可以关注该PR：https://github.com/apache/incubator-seatunnel/pull/2574

- 支持更多数据集成场景 （SeaTunnel Engine）

  用于解决整库同步、表结构变更同步、任务失败影响粒度大等现有引擎不能解决的痛点；

  > 对engine感兴趣的同学可以关注该Umbrella：https://github.com/apache/incubator-seatunnel/issues/2272

- 更简单易用（SeaTunnel Web）

  提供Web界面以DAG/SQL等方式使操作更简单，更加直观的展示Catalog、Connector、Job等；

  接入调度平台，使任务管理更简单；

  > 对Web 感兴趣的同学可以关注我们的Web子项目：https://github.com/apache/incubator-seatunnel-web

### Engine 动机

- Schema Evolution：支持表结构的同步；
- Shared resources：共享资源，如JDBC连接池等；
- Read Sharding：支持分库分表的读取；
- Speed Control：支持在未反压或反压无法传递到Source时，对Source进行读取速率的控制；
- Data Cache：支持Sink出错时，将不可重启的Source数据写入到缓存中；
- Finer fault tolerance：支持更小粒度的容错；

## 参考

1. [Chi: A Scalable and Programmable Control Plane for Distributed Stream Processing Systems](https://shivaram.org/publications/chi-vldb18.pdf)
2. [Time, Clocks and the Ordering of Events in a Distributed System](http://lamport.azurewebsites.net/pubs/pubs.html#time-clocks)
3. [Lightweight Asynchronous Snapshots for Distributed Dataflows](https://arxiv.org/abs/1506.08603)
4. [分布式快照算法: Chandy-Lamport 算法](https://zhuanlan.zhihu.com/p/53482103)
